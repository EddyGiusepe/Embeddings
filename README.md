# Embeddings

![](https://camo.githubusercontent.com/f47de8ec4be8e79ac2c609006dcd5b57cbc9d4d3a2d4e79c23d6282b17f6fc4d/68747470733a2f2f7777772e677374617469632e636f6d2f61696875622f74666875622f756e6976657273616c2d73656e74656e63652d656e636f6465722f6578616d706c652d73696d696c61726974792e706e67)

![image](https://user-images.githubusercontent.com/69597971/221314892-4837e952-ad9e-4be0-a13d-18ee5767c77e.png)


Os `Embeddings` são uma técnica usada em aprendizado de máquina e processamento de linguagem natural (NLP) para representar palavras, frases ou documentos como vetores de números. Essa representação vetorial permite que os computadores processem a linguagem natural de maneira mais eficiente e precisa.

Os `Embeddings` são criados usando algoritmos que analisam as relações entre as palavras em um corpus (conjunto de textos) e mapeiam cada palavra para um vetor de números em um espaço `n-dimensional`. Esse vetor representa as características semânticas e sintáticas da palavra, como seu significado e contexto. As palavras que são semanticamente semelhantes têm vetores semelhantes no `Espaço de Embedding`.

Os Embeddings são usados em diversas tarefas de NLP, como `classificação de texto`, `tradução automática`, `sumarização de texto` e `reconhecimento de entidades nomeadas (NER)`. Eles são especialmente úteis em tarefas que envolvem grande quantidade de dados, pois permitem que os modelos de aprendizado de máquina processem informações com mais eficiência e precisão do que se usassem as próprias palavras como entrada.

Os Embeddings são de tamanho fixo. Quando as palavras são convertidas em embeddings, elas são representadas por um vetor de números de tamanho fixo, ou seja, cada embedding tem um número fixo de dimensões. O tamanho do vetor é um hiperparâmetro escolhido pelo projetista do modelo e pode variar de acordo com a tarefa e o tamanho do corpus usado para criar os embeddings.









Thanks God! 
